# v0.2 规划 - 模式系统 + 流式输出

## 功能需求

### 1. 模式管理
- 定义多个模式（普通聊天、前端导师、代码审查等）
- 模式切换功能
- 模式配置（系统提示、上下文长度等）

### 2. 上下文管理
- 滑动窗口（只发送最近 N 条消息）
- System Prompt 注入
- 上下文长度优化

### 3. 用户界面
- 模式选择器
- 当前模式显示
- 模式切换动画

### 4. 流式输出
- 实时显示 AI 回复内容
- 打字机效果
- 流式数据解析和渲染

## 实现步骤

### 步骤 1：类型定义
- 定义 Mode 接口
- 定义 StreamChunk 接口
- 扩展 AppState 类型

### 步骤 2：模式配置
- 定义 MODES 常量
- 配置预定义模式（普通聊天、前端导师、代码审查）

### 步骤 3：上下文管理
- 创建 getContext 函数
- 实现 System Prompt 注入
- 实现滑动窗口逻辑

### 步骤 4：API 服务改造
- 创建 StreamChunk 类型
- 实现 sendMessageStream 函数
- 处理流式响应

### 步骤 5：UI 组件
- 创建 ModeSelector 组件
- 实现模式切换 UI

### 步骤 6：状态管理更新
- 更新 AppState 类型
- 实现流式输出的状态更新逻辑
- 集成模式切换和流式输出

## 技术要点

### 1. System Prompt 的作用
- 定义角色：告诉模型"你是谁"
- 设定行为：规定模型如何回答
- 控制范围：限制模型的回答边界

### 2. 上下文窗口
- 为什么需要：模型有 token 限制（通常是 4096 或 8192）
- 滑动窗口：只保留最近 N 条对话
- 优化策略：根据模式动态调整上下文长度

### 3. 流式输出
- 原理：使用 ReadableStream API
- 优势：即时反馈、更好的体验、性能优化
- 实现要点：getReader、逐块解析、实时更新状态

## 测试验证

### 测试场景
1. **普通聊天模式**：友好的回答
2. **前端导师模式**：专业、有代码示例、引导深入
3. **代码审查模式**：严格审查、指出问题、给出建议
4. **流式输出**：内容逐字显示，有打字机效果

### 验证点
- 模式切换后，回答风格明显不同
- 上下文正确传递
- System Prompt 不泄露给用户
- 上下文长度控制有效
- 流式输出实时显示
- 流式输出过程中可以正确处理中断和错误

## 学习收获

1. 系统提示工程：如何设计有效的 system prompt
2. 上下文管理：如何优化对话历史
3. 前端主导 AI：如何通过前端控制模型行为
4. 状态管理进阶：复杂状态的拆分和管理
5. 流式数据处理：如何处理和渲染流式 API 响应
6. 异步编程：使用 ReadableStream API 和回调函数处理异步流