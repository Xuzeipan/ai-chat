# AI 供应商适配器接口案例

## 基础实现

### 1. 定义适配器接口

```typescript
// src/services/ai/base.ts

export interface AIModel {
  id: string;
  name: string;
  maxTokens: number;
  supportStream: boolean;
  supportVision: boolean;
}

export interface ChatMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

export interface ChatConfig {
  model: string;
  temperature?: number;
  maxTokens?: number;
  apiKey: string;
  baseUrl?: string;
}

export interface ChatResponse {
  content: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

export interface StreamCallbacks {
  onChunk: (chunk: string) => void;
  onComplete: () => void;
  onError: (error: Error) => void;
}

export interface AIAAdapter {
  // 获取支持的模型列表
  getModels(): Promise<AIModel[]>;

  // 非流式对话
  chat(messages: ChatMessage[], config: ChatConfig): Promise<ChatResponse>;

  // 流式对话
  chatStream(
    messages: ChatMessage[],
    config: ChatConfig,
    callbacks: StreamCallbacks
  ): Promise<void>;

  // 估算 token 数量
  estimateTokens(text: string): number;
}
```

### 2. OpenAI 适配器实现

```typescript
// src/services/ai/openai.adapter.ts
import OpenAI from 'openai';
import {
  AIAAdapter,
  AIModel,
  ChatMessage,
  ChatConfig,
  ChatResponse,
  StreamCallbacks
} from './base.js';

export class OpenAIAdapter implements AIAAdapter {
  async getModels(): Promise<AIModel[]> {
    return [
      {
        id: 'gpt-4',
        name: 'GPT-4',
        maxTokens: 8192,
        supportStream: true,
        supportVision: true
      },
      {
        id: 'gpt-4-turbo',
        name: 'GPT-4 Turbo',
        maxTokens: 128000,
        supportStream: true,
        supportVision: true
      },
      {
        id: 'gpt-3.5-turbo',
        name: 'GPT-3.5 Turbo',
        maxTokens: 16385,
        supportStream: true,
        supportVision: false
      }
    ];
  }

  async chat(
    messages: ChatMessage[],
    config: ChatConfig
  ): Promise<ChatResponse> {
    const client = new OpenAI({
      apiKey: config.apiKey,
      baseURL: config.baseUrl || 'https://api.openai.com/v1'
    });

    const response = await client.chat.completions.create({
      model: config.model,
      messages: messages as OpenAI.Chat.ChatCompletionMessageParam[],
      temperature: config.temperature ?? 0.7,
      max_tokens: config.maxTokens
    });

    return {
      content: response.choices[0]?.message?.content || '',
      usage: response.usage
        ? {
            promptTokens: response.usage.prompt_tokens,
            completionTokens: response.usage.completion_tokens,
            totalTokens: response.usage.total_tokens
          }
        : undefined
    };
  }

  async chatStream(
    messages: ChatMessage[],
    config: ChatConfig,
    callbacks: StreamCallbacks
  ): Promise<void> {
    const client = new OpenAI({
      apiKey: config.apiKey,
      baseURL: config.baseUrl || 'https://api.openai.com/v1'
    });

    try {
      const stream = await client.chat.completions.create({
        model: config.model,
        messages: messages as OpenAI.Chat.ChatCompletionMessageParam[],
        temperature: config.temperature ?? 0.7,
        max_tokens: config.maxTokens,
        stream: true
      });

      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content;
        if (content) {
          callbacks.onChunk(content);
        }
      }

      callbacks.onComplete();
    } catch (error) {
      callbacks.onError(error as Error);
    }
  }

  estimateTokens(text: string): number {
    // 简单估算：英文 1 token ≈ 4 字符，中文 1 token ≈ 1 字符
    const chineseChars = (text.match(/[\u4e00-\u9fa5]/g) || []).length;
    const otherChars = text.length - chineseChars;
    return Math.ceil(chineseChars + otherChars / 4);
  }
}
```

### 3. Ollama 适配器实现

```typescript
// src/services/ai/ollama.adapter.ts
import {
  AIAAdapter,
  AIModel,
  ChatMessage,
  ChatConfig,
  ChatResponse,
  StreamCallbacks
} from './base.js';

export class OllamaAdapter implements AIAAdapter {
  async getModels(): Promise<AIModel[]> {
    // 从 Ollama API 获取可用模型
    try {
      const response = await fetch('http://localhost:11434/api/tags');
      const data = await response.json();

      return data.models.map((m: { name: string }) => ({
        id: m.name,
        name: m.name,
        maxTokens: 8192,
        supportStream: true,
        supportVision: false
      }));
    } catch {
      // 如果无法连接，返回默认列表
      return [
        {
          id: 'qwen2.5-coder:7b',
          name: 'Qwen 2.5 Coder 7B',
          maxTokens: 8192,
          supportStream: true,
          supportVision: false
        }
      ];
    }
  }

  async chat(
    messages: ChatMessage[],
    config: ChatConfig
  ): Promise<ChatResponse> {
    const baseUrl = config.baseUrl || 'http://localhost:11434';

    const response = await fetch(`${baseUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: config.model,
        messages,
        stream: false,
        options: {
          temperature: config.temperature ?? 0.7
        }
      })
    });

    const data = await response.json();

    return {
      content: data.message?.content || ''
    };
  }

  async chatStream(
    messages: ChatMessage[],
    config: ChatConfig,
    callbacks: StreamCallbacks
  ): Promise<void> {
    const baseUrl = config.baseUrl || 'http://localhost:11434';

    try {
      const response = await fetch(`${baseUrl}/api/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: config.model,
          messages,
          stream: true,
          options: {
            temperature: config.temperature ?? 0.7
          }
        })
      });

      const reader = response.body?.getReader();
      if (!reader) {
        throw new Error('无法读取响应流');
      }

      const decoder = new TextDecoder();

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(Boolean);

        for (const line of lines) {
          try {
            const data = JSON.parse(line);
            if (data.message?.content) {
              callbacks.onChunk(data.message.content);
            }
            if (data.done) {
              callbacks.onComplete();
              return;
            }
          } catch {
            // 忽略解析错误的行
          }
        }
      }

      callbacks.onComplete();
    } catch (error) {
      callbacks.onError(error as Error);
    }
  }

  estimateTokens(text: string): number {
    return Math.ceil(text.length / 4);
  }
}
```

### 4. 适配器工厂

```typescript
// src/services/ai/index.ts
import { AIAAdapter } from './base.js';
import { OpenAIAdapter } from './openai.adapter.js';
import { OllamaAdapter } from './ollama.adapter.js';

export type ProviderType =
  | 'openai'
  | 'claude'
  | 'gemini'
  | 'kimi'
  | 'minimax'
  | 'deepseek'
  | 'zhipu'
  | 'ollama';

const adapters: Record<ProviderType, () => AIAAdapter> = {
  openai: () => new OpenAIAdapter(),
  claude: () => new OpenAIAdapter(), // Claude 也兼容 OpenAI 格式
  gemini: () => new OpenAIAdapter(), // 先用 OpenAI 兼容模式
  kimi: () => new OpenAIAdapter(),   // Moonshot 兼容 OpenAI 格式
  minimax: () => new OpenAIAdapter(),
  deepseek: () => new OpenAIAdapter(),
  zhipu: () => new OpenAIAdapter(),
  ollama: () => new OllamaAdapter()
};

export function getAdapter(provider: ProviderType): AIAAdapter {
  const createAdapter = adapters[provider];
  if (!createAdapter) {
    throw new Error(`不支持的供应商: ${provider}`);
  }
  return createAdapter();
}

export { AIAAdapter, AIModel, ChatMessage, ChatConfig, ChatResponse };
```

## 使用示例

### 使用适配器发送消息

```typescript
import { getAdapter } from './services/ai/index.js';

const adapter = getAdapter('openai');

// 获取可用模型
const models = await adapter.getModels();
console.log(models);

// 发送消息
const response = await adapter.chat(
  [
    { role: 'system', content: '你是一个 helpful 助手' },
    { role: 'user', content: '你好' }
  ],
  {
    model: 'gpt-3.5-turbo',
    apiKey: 'your-api-key'
  }
);

console.log(response.content);

// 流式输出
await adapter.chatStream(
  [{ role: 'user', content: '讲个故事' }],
  {
    model: 'gpt-3.5-turbo',
    apiKey: 'your-api-key'
  },
  {
    onChunk: (chunk) => process.stdout.write(chunk),
    onComplete: () => console.log('\n完成'),
    onError: (err) => console.error('错误:', err)
  }
);
```

## 原理解释

### 适配器模式

适配器模式将一个类的接口转换成客户希望的另外一个接口。

**为什么使用适配器？**
- 不同 AI 供应商 API 格式不同
- 通过适配器统一接口，上层代码无需关心具体供应商
- 新增供应商只需添加适配器，无需修改业务代码

### 流式响应处理

- **OpenAI**: 使用官方 SDK，支持异步迭代 `for await`
- **Ollama**: 使用原生 fetch 和 ReadableStream，手动解析 NDJSON

## 进阶功能

### 添加重试机制

```typescript
async function withRetry<T>(
  fn: () => Promise<T>,
  maxRetries = 3
): Promise<T> {
  let lastError: Error;

  for (let i = 0; i < maxRetries; i++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error as Error;
      await new Promise((r) => setTimeout(r, 1000 * (i + 1)));
    }
  }

  throw lastError;
}

// 使用
const response = await withRetry(() => adapter.chat(messages, config));
```

## 你的任务

1. 安装依赖 `pnpm add openai @anthropic-ai/sdk @google/generative-ai`
2. 创建 `src/services/ai/base.ts` 定义接口
3. 创建 `src/services/ai/openai.adapter.ts` OpenAI 适配器
4. 创建 `src/services/ai/ollama.adapter.ts` Ollama 适配器
5. 创建 `src/services/ai/index.ts` 适配器工厂
6. 测试调用不同适配器

先完成基础框架，后续再逐个添加其他供应商的详细实现。

完成后告诉我，我帮你检查。
