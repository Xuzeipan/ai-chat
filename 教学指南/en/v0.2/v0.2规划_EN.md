# v0.2 Planning - Mode System + Streaming Output

## Feature Requirements

### 1. Mode System
- Define multiple modes (General Chat, Frontend Mentor, Code Reviewer, etc.)
- Mode switching functionality
- Mode configuration (system prompt, context length, etc.)

### 2. Context Management
- Sliding window (send only the last N messages)
- System Prompt injection
- Context length optimization

### 3. Streaming Output
- Real-time display of AI response content
- Typewriter effect
- Streaming data parsing and rendering

## Implementation Steps (By Feature Modules)

### Feature Module 1: Mode System

#### Step 1: Type Definitions
- Define Mode interface
- Extend AppState type (add currentMode and modes)

#### Step 2: Mode Configuration
- Define MODES constant
- Configure predefined modes (General Chat, Frontend Mentor, Code Reviewer)

#### Step 3: ModeSelector Component
- Create mode selector UI
- Implement mode switching interaction

#### Step 4: Integrate Mode Switching
- Integrate ModeSelector in App
- Implement mode switching state management

### Feature Module 2: Context Management

#### Step 5: getContext Function
- Implement context management logic
- Implement System Prompt injection
- Implement sliding window

#### Step 6: Integrate Context Management
- Use context when sending messages
- Verify context is passed correctly

### Feature Module 3: Streaming Output

#### Step 7: Streaming Type Definitions
- Define StreamChunk interface

#### Step 8: API Streaming Transformation
- Implement sendMessageStream function
- Handle streaming responses

#### Step 9: Integrate Streaming Output
- Integrate streaming updates in state management
- Implement real-time display effect

## Technical Key Points

### 1. Role of System Prompt
- Define role: Tell the model "who you are"
- Set behavior: Define how the model should answer
- Control scope: Limit the model's answer boundaries

### 2. Context Window
- Why it's needed: Models have token limits (usually 4096 or 8192)
- Sliding window: Keep only the last N messages
- Optimization strategy: Dynamically adjust context length based on mode

### 3. Streaming Output
- Principle: Use ReadableStream API
- Advantages: Immediate feedback, better experience, performance optimization
- Implementation points: getReader, chunk-by-chunk parsing, real-time state update

## Testing Verification

### Test Scenarios
1. **Mode Switching**: After switching modes, answer styles should be significantly different
2. **Context Passing**: Verify context is correctly passed to API
3. **System Prompt**: Verify System Prompt is not leaked to users
4. **Context Length**: Verify context length control is effective
5. **Streaming Output**: Content displayed character by character with typewriter effect
6. **Error Handling**: Can correctly handle interruptions and errors during streaming output

### Verification Points
- ✅ Mode switching works correctly
- ✅ Context is passed correctly
- ✅ System Prompt is not leaked to users
- ✅ Context length control is effective
- ✅ Streaming output displays in real-time
- ✅ Streaming output error handling is correct

## Learning Outcomes

1. Modular development: Organize code by feature modules
2. Prompt engineering: How to design effective system prompts
3. Context management: How to optimize conversation history
4. Frontend-led AI: How to control model behavior through frontend
5. Advanced state management: Complex state splitting and management
6. Streaming data processing: How to handle and render streaming API responses
7. Asynchronous programming: Using ReadableStream API and callback functions for async stream handling
